{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f91ff59-f346-4557-bcfe-377d7eca5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vivi 2.0  ‚Äì chat + smart video clipping  \n",
    "# ----------------------------------------------------------  \n",
    "# Imports  \n",
    "# ---------------------------------------------------------- \n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, scrolledtext, ttk\n",
    "import whisper\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "import tempfile\n",
    "import uuid\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66190df1-022e-433e-bb60-221e53daca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_srt_time(t):\n",
    "    try:\n",
    "        h, m, s_ms = t.split(\":\")\n",
    "        if \",\" in s_ms:\n",
    "            s, ms = s_ms.split(\",\")\n",
    "            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000\n",
    "        else:\n",
    "            return int(h) * 3600 + int(m) * 60 + float(s_ms)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        m, s = t.split(\":\")\n",
    "        return int(m) * 60 + float(s)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        return float(t)\n",
    "    except:\n",
    "        raise ValueError(f\"Unrecognized timestamp format: {t}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02}:{m:02}:{s:06.3f}\".replace(\".\", \",\")\n",
    "\n",
    "# ------------------ FFmpeg Functions ------------------\n",
    "\n",
    "def concatenate_videos(video_paths, output_filepath):\n",
    "    try:\n",
    "        if not video_paths:\n",
    "            return \"‚ùå Error: No video clips provided.\", None\n",
    "\n",
    "        ffmpeg_path = \"C:\\\\ffmpeg\\\\ffmpeg.exe\"  \n",
    "\n",
    "        # Create unique list file in temp dir\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        list_file_path = os.path.join(temp_dir, f\"concat_list_{uuid.uuid4().hex[:8]}.txt\")\n",
    "\n",
    "        # Write the list of files to concatenate\n",
    "        with open(list_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for path in video_paths:\n",
    "                f.write(f\"file '{path.replace('\\\\', '/')}'\\n\")\n",
    "\n",
    "        # Run ffmpeg concat\n",
    "        command = [\n",
    "            ffmpeg_path, \"-y\",\n",
    "            \"-f\", \"concat\", \"-safe\", \"0\",\n",
    "            \"-i\", list_file_path,\n",
    "            \"-c\", \"copy\", output_filepath\n",
    "        ]\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "        # Cleanup list file\n",
    "        os.remove(list_file_path)\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            return f\"‚ùå FFmpeg error:\\n{result.stderr}\", None\n",
    "\n",
    "        return f\"‚úÖ Concatenated video saved to: {output_filepath}\", output_filepath\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Exception during concatenation: {str(e)}\", None\n",
    "\n",
    "\n",
    "#def concatenate_videos(video_paths, output_path):\n",
    " #   ffmpeg_path = 'C:\\\\ffmpeg'\n",
    "  #  try:\n",
    "   #     if not video_paths:\n",
    "    #        return \"Error: No video clips provided.\", None\n",
    "\n",
    "     #   list_file = \"concat_list.txt\"\n",
    "      #  with open(list_file, \"w\") as f:\n",
    "       #     for path in video_paths:\n",
    "        #        f.write(f\"file '{path}'\\n\")\n",
    "\n",
    "#        command = [\n",
    " #           ffmpeg_path, \"-y\", \"-f\", \"concat\", \"-safe\", \"0\",\n",
    "  #          \"-i\", list_file, \"-c\", \"copy\", output_path\n",
    "   #     ]\n",
    "\n",
    "    #    result = subprocess.run(command, capture_output=True, text=True)\n",
    "     #   os.remove(list_file)\n",
    "      #  if result.returncode != 0:\n",
    "       #     return f\"Error concatenating: {result.stderr}\", None\n",
    "        #return f\"‚úÖ Concatenated video saved to {output_path}\", output_path\n",
    "\n",
    "#    except Exception as e:\n",
    " #       return f\"Error concatenating videos: {str(e)}\", None\n",
    "\n",
    "# ------------------ LLM Setup ------------------\n",
    "llm = OllamaLLM(model='gemma3:1B')\n",
    "\n",
    "general_template = \"\"\"\n",
    "You are Vivi, an expert and friendly video assistant chatbot.\n",
    "You are having an ongoing conversation with the user. You have access to a full transcript of a video. If the user‚Äôs question is about the video, answer helpfully and refer to timestamps if relevant.\n",
    "If the question is general and not related to the video, just respond helpfully like a normal assistant.\n",
    "---\n",
    "Conversation History:\n",
    "{context}\n",
    "---\n",
    "Full Transcript of the Video:\n",
    "{transcript}\n",
    "---\n",
    "User:\n",
    "{question}\n",
    "---\n",
    "Vivi:\n",
    "\"\"\"\n",
    "\n",
    "clipping_template = \"\"\"\n",
    "You are an expert video analysis assistant. Given a user query and the transcript of a video with timestamps, identify all timestamp ranges where the video content is relevant to the query. Return the result as a list of timestamp ranges (start and end times in seconds) and a brief explanation of why each range is relevant.\n",
    "\n",
    "Return timestamps as plain numbers with two decimal places (e.g., 31.00, 45.00) without brackets or other characters. Ensure the format is consistent. Ensure end_time is greater than start_time and both are non-negative.\n",
    "\n",
    "Query: {query}\n",
    "Transcript: {transcript}\n",
    "\n",
    "Return the result in the following format and each range and its explanation must be on separate lines:\n",
    "- Range: start_time - end_time\n",
    "- Range: start_time - end_time\n",
    "  Relevance: [Brief explanation of why this range is relevant to the query]\n",
    "\"\"\"\n",
    "\n",
    "prompt_general = ChatPromptTemplate.from_template(general_template)\n",
    "prompt_clipping = ChatPromptTemplate.from_template(clipping_template)\n",
    "chain_general = prompt_general | llm\n",
    "chain_clipping = prompt_clipping | llm\n",
    "\n",
    "# ------------------ Vivi GUI Class ------------------\n",
    "class ViviChatbot:\n",
    "    def __init__(self):\n",
    "        self.video_path = \"\"\n",
    "        self.context = \"\"\n",
    "        self.transcript_segments = []\n",
    "        self.full_transcript_text = \"\"\n",
    "        self.cap = None\n",
    "        self.playing = False\n",
    "        self.current_frame = 0\n",
    "        self.seek_scale = None\n",
    "        self.audio_process = None\n",
    "\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Vivi Video Chatbot\")\n",
    "\n",
    "        self.chat_display = scrolledtext.ScrolledText(self.root, wrap=tk.WORD, width=80, height=30, font=(\"Arial\", 12))\n",
    "        self.chat_display.pack(padx=10, pady=10)\n",
    "\n",
    "        self.user_entry = tk.Entry(self.root, font=(\"Arial\", 12))\n",
    "        self.user_entry.pack(fill=tk.X, padx=10, pady=(0, 10))\n",
    "        self.user_entry.bind(\"<Return>\", lambda e: self.send_message())\n",
    "\n",
    "        self.send_btn = tk.Button(self.root, text=\"Send\", font=(\"Arial\", 12), command=self.send_message)\n",
    "        self.send_btn.pack(pady=(0, 10))\n",
    "\n",
    "        btn_frame = tk.Frame(self.root)\n",
    "        btn_frame.pack()\n",
    "\n",
    "        self.browse_btn = tk.Button(btn_frame, text=\"üìÇ Upload Video\", command=self.browse_video)\n",
    "        self.browse_btn.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.preview_btn = tk.Button(btn_frame, text=\"‚ñ∂Ô∏è Preview Video\", command=self.play_video)\n",
    "        self.preview_btn.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.progress_var = tk.DoubleVar()\n",
    "        self.progress_bar = ttk.Progressbar(self.root, orient=\"horizontal\", mode=\"determinate\", variable=self.progress_var)\n",
    "        self.progress_bar.pack(fill=tk.X, padx=10, pady=5)\n",
    "\n",
    "        self.chat_display.insert(tk.END, \" Welcome to the Vivi Video Chatbot! Type 'exit' to quit.\\n For getting your video clipped, enter your query in the format:\\n Video clipping:<query>\")\n",
    "        self.chat_display.yview(tk.END)\n",
    "\n",
    "    \n",
    "    \n",
    "    def format_time(self, seconds):\n",
    "        h = int(seconds // 3600)\n",
    "        m = int((seconds % 3600) // 60)\n",
    "        s = seconds % 60\n",
    "        return f\"{h:02}:{m:02}:{s:06.3f}\".replace(\".\", \",\")\n",
    "\n",
    "    def clip_video(self, start_time, end_time):\n",
    "        try:\n",
    "            # Validate time range\n",
    "            if start_time >= end_time:\n",
    "                self.chat_display.insert(tk.END, f\"\\n‚ö† Invalid clip range: start ({start_time}) >= end ({end_time})\\n\")\n",
    "                return \" Invalid time range\", None\n",
    "            \n",
    "            # Use a unique filename to avoid collisions\n",
    "            clip_filename = f\"clip_{uuid.uuid4().hex[:8]}.mp4\"\n",
    "            output_path = os.path.join(tempfile.gettempdir(), clip_filename)\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                os.remove(output_path)\n",
    "\n",
    "            subprocess.run([\n",
    "                \"ffmpeg\", \"-y\",\n",
    "                \"-ss\", str(start_time),\n",
    "                \"-to\", str(end_time),\n",
    "                \"-i\", self.video_path,\n",
    "                \"-c\", \"copy\",\n",
    "                output_path\n",
    "            ], check=True)\n",
    "\n",
    "            self.chat_display.yview(tk.END)\n",
    "            return f\"üé¨ Video clip saved to {output_path}\", output_path\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.chat_display.insert(tk.END, f\"\\n‚ùå FFmpeg failed: {e}\\n\")\n",
    "        except PermissionError as e:\n",
    "            self.chat_display.insert(tk.END, f\"\\n‚ùå Permission denied: {e}\\n\")\n",
    "        except Exception as e:\n",
    "            self.chat_display.insert(tk.END, f\"\\n‚ùå Error clipping video: {e}\\n\")\n",
    "    \n",
    "    def transcribe_video(self, video_path):\n",
    "        base = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        dir_ = os.path.dirname(video_path)\n",
    "        txt_path = os.path.join(dir_, f\"{base}.txt\")\n",
    "        srt_path = os.path.join(dir_, f\"{base}.srt\")\n",
    "\n",
    "        if os.path.exists(txt_path) and os.path.exists(srt_path):\n",
    "            self.chat_display.insert(tk.END, \"‚úÖ Transcript and subtitles found.\\n\")\n",
    "            with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                blocks = f.read().strip().split(\"\\n\\n\")\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                timed_transcript=f.read().strip()\n",
    "            segments = []\n",
    "            for block in blocks:\n",
    "                lines = block.split(\"\\n\")\n",
    "                if len(lines) >= 3:\n",
    "                    times = lines[1].split(\" --> \")\n",
    "                    start = _parse_srt_time(times[0])\n",
    "                    end = _parse_srt_time(times[1])\n",
    "                    text = \" \".join(lines[2:])\n",
    "                    segments.append({\"start\": start, \"end\": end, \"text\": text})\n",
    "            return timed_transcript, segments\n",
    "\n",
    "        self.chat_display.insert(tk.END, \"üîç Running Whisper transcription...\\n\")\n",
    "        model = whisper.load_model(\"medium\")\n",
    "        result = model.transcribe(video_path, verbose=True)\n",
    "        segments = []\n",
    "        timed_transcript=\"\"\n",
    "        for seg in result['segments']:\n",
    "            segments.append({\"start\": seg['start'], \"end\": seg['end'], \"text\": seg['text']})\n",
    "            timed_transcript+=f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\\n\"\n",
    "\n",
    "        with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, seg in enumerate(segments):\n",
    "                f.write(f\"{i+1}\\n{format_time(seg['start'])} --> {format_time(seg['end'])}\\n{seg['text']}\\n\\n\")\n",
    "\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(timed_transcript)\n",
    "\n",
    "        return timed_transcript, segments\n",
    "\n",
    "    def send_message(self):\n",
    "        user_input = self.user_entry.get()\n",
    "        if user_input.strip().lower() == \"exit\":\n",
    "            self.root.destroy()\n",
    "            return\n",
    "\n",
    "        self.chat_display.insert(tk.END, f\"You: {user_input}\\n\")\n",
    "        self.user_entry.delete(0, tk.END)\n",
    "        self.user_entry.config(state=\"disabled\")\n",
    "        self.send_btn.config(state=\"disabled\")\n",
    "\n",
    "        def run_bot():\n",
    "            if user_input.lower().startswith(\"video clipping:\"):\n",
    "                query = user_input[len(\"video clipping:\"):].strip()\n",
    "                response = chain_clipping.invoke({\"query\": query, \"transcript\": self.transcript_segments})\n",
    "                self.chat_display.insert(tk.END, f\"\\nVivi:\")\n",
    "                video_clips = []\n",
    "                for line in response.strip().split(\"\\n\"):\n",
    "                    if line.startswith(\"- Range: \"):\n",
    "                        parts = line[len(\"- Range: \"):].split(\" - \")\n",
    "                        if len(parts) == 2:\n",
    "                            try:\n",
    "                                start = float(parts[0])\n",
    "                                end = float(parts[1])\n",
    "                            except ValueError:\n",
    "                                try:\n",
    "                                    start = _parse_srt_time(parts[0])\n",
    "                                    end = _parse_srt_time(parts[1])\n",
    "                                except Exception as e:\n",
    "                                    self.chat_display.insert(tk.END, f\"\\n‚ö†Ô∏è Failed to parse timestamps: {parts}\")\n",
    "                                    continue\n",
    "                            msg, clip_path = self.clip_video(start, end)\n",
    "                            self.chat_display.insert(tk.END, f\"\\n{msg}\")\n",
    "                            if clip_path:\n",
    "                                video_clips.append(clip_path)\n",
    "                if video_clips:\n",
    "                    msg, out = concatenate_videos(video_clips, output_filepath=os.path.join(os.path.dirname(self.video_path), \"final_output.mp4\"))\n",
    "                    self.chat_display.insert(tk.END, f\"\\n{msg}\\n\")\n",
    "                \n",
    "                    if out:\n",
    "                        for clip in video_clips:\n",
    "                            try:\n",
    "                                os.remove(clip)\n",
    "                            except Exception as e:\n",
    "                                self.chat_display.insert(tk.END, f\"‚ö†Ô∏è Failed to delete {clip}: {e}\\n\")\n",
    "                        self.chat_display.insert(tk.END, f\"üßπ Deleted all temporary clips\\n\")\n",
    "\n",
    "            else:\n",
    "                response = chain_general.invoke({\n",
    "                    \"context\": self.context,\n",
    "                    \"question\": user_input,\n",
    "                    \"transcript\": self.transcript_segments\n",
    "                })\n",
    "                self.chat_display.insert(tk.END, \"Vivi: \")\n",
    "                for word in response.split():\n",
    "                    self.chat_display.insert(tk.END, word + \" \")\n",
    "                    self.chat_display.yview(tk.END)\n",
    "                    time.sleep(0.04)\n",
    "                self.chat_display.insert(tk.END, \"\\n\\n\")\n",
    "                self.context += f\"\\nUser: {user_input}\\nAI: {response}\\n\"\n",
    "            self.user_entry.config(state=\"normal\")\n",
    "            self.send_btn.config(state=\"normal\")\n",
    "            self.user_entry.focus()\n",
    "\n",
    "        threading.Thread(target=run_bot).start()\n",
    "\n",
    "    def browse_video(self):\n",
    "        self.video_path = filedialog.askopenfilename(filetypes=[(\"Video Files\", \"*.mp4 *.mov *.avi\")])\n",
    "        if not self.video_path:\n",
    "            return\n",
    "\n",
    "        self.chat_display.insert(tk.END, f\"\\nüìÅ Selected video: {os.path.basename(self.video_path)}\\n\")\n",
    "\n",
    "        def process_video():\n",
    "            self.full_transcript_text, self.transcript_segments = self.transcribe_video(self.video_path)\n",
    "            self.chat_display.insert(tk.END, \"‚úÖ Transcription completed!\\n\\n\")\n",
    "            self.progress_var.set(0)\n",
    "\n",
    "        threading.Thread(target=process_video).start()\n",
    "\n",
    "    def play_video(self):\n",
    "        # Check if final_output.mp4 exists\n",
    "        final_output = os.path.join(os.path.dirname(self.video_path), \"final_output.mp4\")\n",
    "        if os.path.exists(final_output):\n",
    "            video_to_play = final_output\n",
    "            self.chat_display.insert(tk.END, f\"\\nüéûÔ∏è Playing final_output.mp4\\n\")\n",
    "        elif self.video_path:\n",
    "            video_to_play = self.video_path\n",
    "            self.chat_display.insert(tk.END, f\"\\nüéûÔ∏è Previewing original uploaded video\\n\")\n",
    "        else:\n",
    "            self.chat_display.insert(tk.END, \"\\n‚ö†Ô∏è No video loaded yet. Upload a video first.\\n\")\n",
    "            return\n",
    "\n",
    "        # Kill previous ffplay process if still running\n",
    "        if self.audio_process and self.audio_process.poll() is None:\n",
    "            self.audio_process.terminate()\n",
    "\n",
    "        try:\n",
    "            # ffplay handles both video & audio, synced\n",
    "            self.audio_process = subprocess.Popen([\n",
    "                \"ffplay\", \"-autoexit\", \"-loglevel\", \"quiet\", video_to_play\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            self.chat_display.insert(tk.END, f\"\\n‚ùå Failed to play video with ffplay: {e}\\n\")\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.root.mainloop()\n",
    "\n",
    "# Start the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    app = ViviChatbot()\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff5156-b951-4741-8221-93210594c746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2ceb15-5394-4d32-8578-1ca69392e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import tempfile\n",
    "import uuid\n",
    "import time\n",
    "import whisper\n",
    "import customtkinter as ctk\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from groq import Groq\n",
    "from PIL import Image, ImageTk\n",
    "from customtkinter import CTkImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16d7a1-5ee0-476e-b3a4-1a06ea50fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Utility Functions ------------------\n",
    "def _parse_srt_time(t):\n",
    "    try:\n",
    "        h, m, s_ms = t.split(\":\")\n",
    "        if \",\" in s_ms:\n",
    "            s, ms = s_ms.split(\",\")\n",
    "            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000\n",
    "        else:\n",
    "            return int(h) * 3600 + int(m) * 60 + float(s_ms)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        m, s = t.split(\":\")\n",
    "        return int(m) * 60 + float(s)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        return float(t)\n",
    "    except:\n",
    "        raise ValueError(f\"Unrecognized timestamp format: {t}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02}:{m:02}:{s:06.3f}\".replace(\".\", \",\")\n",
    "\n",
    "# ------------------ FFmpeg Functions ------------------\n",
    "def concatenate_videos(video_paths, output_filepath):\n",
    "    try:\n",
    "        if not video_paths:\n",
    "            return \"‚ùå Error: No video clips provided.\", None\n",
    "        ffmpeg_path = \"C:\\\\ffmpeg\\\\ffmpeg.exe\"\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        list_file_path = os.path.join(temp_dir, f\"concat_list_{uuid.uuid4().hex[:8]}.txt\")\n",
    "        with open(list_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for path in video_paths:\n",
    "                f.write(f\"file '{path.replace('\\\\', '/')}\\n\")\n",
    "        command = [\n",
    "            ffmpeg_path, \"-y\", \"-f\", \"concat\", \"-safe\", \"0\",\n",
    "            \"-i\", list_file_path, \"-c\", \"copy\", output_filepath\n",
    "        ]\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        os.remove(list_file_path)\n",
    "        if result.returncode != 0:\n",
    "            return f\"‚ùå FFmpeg error:\\n{result.stderr}\", None\n",
    "        return f\"‚úÖ Concatenated video saved to: {output_filepath}\", output_filepath\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Exception during concatenation: {str(e)}\", None\n",
    "\n",
    "# ------------------ LLM Setup ------------------\n",
    "llm = OllamaLLM(model='gemma3:1B')\n",
    "\n",
    "general_template = \"\"\"\n",
    "You are Vivi, an expert and friendly video assistant chatbot.\n",
    "You are having an ongoing conversation with the user. You have access to a full transcript of a video. If the user‚Äôs question is about the video, answer helpfully and refer to timestamps if relevant.\n",
    "If the question is general and not related to the video, just respond helpfully like a normal assistant.\n",
    "---\n",
    "Conversation History:\n",
    "{context}\n",
    "---\n",
    "Full Transcript of the Video:\n",
    "{transcript}\n",
    "---\n",
    "User:\n",
    "{question}\n",
    "---\n",
    "Vivi:\n",
    "\"\"\"\n",
    "\n",
    "clipping_template = \"\"\"\n",
    "You are an expert video analysis assistant. Given a user query and the transcript of a video with timestamps, identify all timestamp ranges where the video content is relevant to the query. Return the result as a list of timestamp ranges (start and end times in seconds) and a brief explanation of why each range is relevant.\n",
    "\n",
    "Return timestamps as plain numbers with two decimal places (e.g., 31.00, 45.00) without brackets or other characters. Ensure the format is consistent. Ensure end_time is greater than start_time and both are non-negative.\n",
    "\n",
    "Query: {query}\n",
    "Transcript: {transcript}\n",
    "\n",
    "Return the result in the following format and each range and its explanation must be on separate lines:\n",
    "- Range: start_time - end_time\n",
    "- Range: start_time - end_time\n",
    "  Relevance: [Brief explanation of why this range is relevant to the query]\n",
    "\"\"\"\n",
    "\n",
    "prompt_general = ChatPromptTemplate.from_template(general_template)\n",
    "prompt_clipping = ChatPromptTemplate.from_template(clipping_template)\n",
    "chain_general = prompt_general | llm\n",
    "chain_clipping = prompt_clipping | llm\n",
    "\n",
    "# ------------------ Vivi GUI Class ------------------\n",
    "class ViviChatbot:\n",
    "    def __init__(self):\n",
    "        # Use light appearance mode\n",
    "        ctk.set_appearance_mode(\"light\")\n",
    "        ctk.set_default_color_theme(\"blue\")  # or \"light-blue\" if available\n",
    "\n",
    "        # Create root and set background\n",
    "        self.root = ctk.CTk()\n",
    "        self.root.configure(fg_color=\"#FFFFFF\")  # white background\n",
    "        self.root.title(\"ClipQuery- Video Chatbot\")\n",
    "        self.root.geometry(\"900x650\")\n",
    "\n",
    "        self.video_path = \"\"\n",
    "        self.context = \"\"\n",
    "        self.transcript_segments = []\n",
    "        self.full_transcript_text = \"\"\n",
    "        self.audio_process = None\n",
    "\n",
    "        # Chat display frame\n",
    "        self.chat_frame = ctk.CTkScrollableFrame(self.root, width=850, height=450, fg_color=\"#FFFFFF\")\n",
    "        self.chat_frame.pack(padx=10, pady=(20, 10), fill=\"both\", expand=True)\n",
    "        self.scrollable_frame = self.chat_frame\n",
    "\n",
    "        # Entry field frame\n",
    "        self.entry_frame = ctk.CTkFrame(self.root, fg_color=\"#FFFFFF\")\n",
    "        self.entry_frame.pack(pady=(0, 10), fill=\"x\", padx=20)\n",
    "\n",
    "        self.user_entry = ctk.CTkEntry(self.entry_frame, placeholder_text=\"Ask Vivi...\", width=600)\n",
    "        self.user_entry.pack(side=\"left\", fill=\"x\", expand=True, padx=(0, 10))\n",
    "        self.user_entry.bind(\"<Return>\", lambda e: self.send_message())\n",
    "\n",
    "        self.send_btn = ctk.CTkButton(self.entry_frame, text=\"Send\", command=self.send_message, width=80, fg_color=\"#1a2238\")\n",
    "        self.send_btn.pack(side=\"right\")\n",
    "\n",
    "        # Buttons for upload, input video, and final video\n",
    "        self.btn_frame = ctk.CTkFrame(self.root, fg_color=\"#FFFFFF\")\n",
    "        self.btn_frame.pack(pady=(0, 10))\n",
    "\n",
    "        self.upload_btn = ctk.CTkButton(self.btn_frame, text=\"üìÇ Upload Video\", command=self.browse_video, fg_color=\"#1a2238\", width=160, height=40, font=(\"Arial\", 16))\n",
    "        self.upload_btn.pack(side=\"left\", padx=5)\n",
    "\n",
    "        self.input_video_btn = ctk.CTkButton(self.btn_frame, text=\"‚ñ∂Ô∏è Input Video\", command=self.play_input_video, fg_color=\"#1a2238\", width=160, height=40, font=(\"Arial\", 16))\n",
    "        self.input_video_btn.pack(side=\"left\", padx=5)\n",
    "\n",
    "        self.final_video_btn = ctk.CTkButton(self.btn_frame, text=\"üé¨ Final Video\", command=self.play_final_video, fg_color=\"#1a2238\", width=160, height=40, font=(\"Arial\", 16))\n",
    "        self.final_video_btn.pack(side=\"left\", padx=5)\n",
    "\n",
    "        # Progress bar\n",
    "        self.progress_var = tk.DoubleVar()\n",
    "        self.progress_bar = ttk.Progressbar(self.root, orient=\"horizontal\", mode=\"determinate\", variable=self.progress_var)\n",
    "        self.progress_bar.pack(fill=\"x\", padx=20, pady=(0, 10))\n",
    "\n",
    "        # Initial welcome message\n",
    "        self.display_message(\"system\", \"üé¨ Welcome to Vivi!\\nType 'exit' to quit.\\nUse format `Video clipping:<your query>` for semantic editing.\")\n",
    "\n",
    "    def typewriter_effect(self, sender, message):\n",
    "        # Outer frame to control side alignment\n",
    "        outer_frame = ctk.CTkFrame(self.scrollable_frame, fg_color=\"#FFFFFF\")\n",
    "        outer_frame.pack(fill=\"x\", padx=10, pady=5)\n",
    "\n",
    "        # Inner chat bubble with side-specific color\n",
    "        bubble_frame = ctk.CTkFrame(\n",
    "            outer_frame,\n",
    "            fg_color=\"#3a0e2e\" if sender.lower() == \"vivi\" else \"#1B263B\",\n",
    "            corner_radius=12\n",
    "        )\n",
    "        if sender.lower() == \"vivi\":\n",
    "            bubble_frame.pack(anchor=\"w\", padx=5)\n",
    "        else:\n",
    "            bubble_frame.pack(anchor=\"e\", padx=5)\n",
    "\n",
    "        # Name label (optional, can be hidden if not needed)\n",
    "        name_label = ctk.CTkLabel(\n",
    "            bubble_frame,\n",
    "            text=sender + \":\",\n",
    "            font=(\"Arial\", 12, \"bold\"),\n",
    "            text_color=\"#F0F0F0\"\n",
    "        )\n",
    "        name_label.pack(anchor=\"w\", padx=8, pady=(6, 0))\n",
    "\n",
    "        # Message label for typewriter text\n",
    "        message_label = ctk.CTkLabel(\n",
    "            bubble_frame,\n",
    "            text=\"\",\n",
    "            font=(\"Arial\", 18, \"normal\"),\n",
    "            wraplength=500,\n",
    "            justify=\"left\",\n",
    "            text_color=\"#F0F0F0\"\n",
    "        )\n",
    "        message_label.pack(anchor=\"w\", padx=8, pady=(0, 8))\n",
    "\n",
    "        # Scroll to bottom before typing\n",
    "        self.root.update_idletasks()\n",
    "        self.chat_frame._parent_canvas.yview_moveto(1.0)\n",
    "\n",
    "        # Typewriter animation\n",
    "        current_text = \"\"\n",
    "        for word in message.split():\n",
    "            current_text += word + \" \"\n",
    "            message_label.configure(text=current_text)\n",
    "            self.root.update_idletasks()\n",
    "            self.chat_frame._parent_canvas.yview_moveto(1.0)\n",
    "            time.sleep(0.04)\n",
    "    \n",
    "    def display_message(self, sender, message):\n",
    "        outer_frame = ctk.CTkFrame(self.chat_frame, fg_color=\"#FFFFFF\")\n",
    "        outer_frame.pack(fill=\"x\", padx=10, pady=5)\n",
    "\n",
    "        bubble = ctk.CTkFrame(\n",
    "            outer_frame,\n",
    "            corner_radius=15,\n",
    "            fg_color=\"#3a0e2e\" if sender.lower() != \"user\" else \"#1a2238\"\n",
    "        )\n",
    "\n",
    "        if sender.lower() == \"user\":\n",
    "            bubble.pack(anchor=\"e\", padx=5)\n",
    "        else:\n",
    "            bubble.pack(anchor=\"w\", padx=5)\n",
    "\n",
    "        label = ctk.CTkLabel(\n",
    "            bubble,\n",
    "            text=f\"{message}\",\n",
    "            wraplength=700,\n",
    "            font=(\"Arial\", 18, \"normal\"),\n",
    "            justify=\"left\",\n",
    "            text_color=\"#F0F0F0\"\n",
    "        )\n",
    "        label.pack(anchor=\"w\", padx=10, pady=5)\n",
    "\n",
    "\n",
    "    def clip_video(self, start_time, end_time):\n",
    "        try:\n",
    "            if start_time >= end_time:\n",
    "                self.display_message(\"system\", f\"‚ö† Invalid clip range: {start_time} >= {end_time}\")\n",
    "                return \"Invalid time range\", None\n",
    "            clip_filename = f\"clip_{uuid.uuid4().hex[:8]}.mp4\"\n",
    "            output_path = os.path.join(tempfile.gettempdir(), clip_filename)\n",
    "            subprocess.run([\n",
    "                \"ffmpeg\", \"-y\", \"-ss\", str(start_time), \"-to\", str(end_time),\n",
    "                \"-i\", self.video_path, \"-c\", \"copy\", output_path\n",
    "            ], check=True)\n",
    "            return f\"üé¨ Video clip saved to {output_path}\", output_path\n",
    "        except Exception as e:\n",
    "            self.display_message(\"system\", f\"‚ùå Error clipping video: {e}\")\n",
    "\n",
    "    def transcribe_video(self, video_path):\n",
    "        base = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        dir_ = os.path.dirname(video_path)\n",
    "        txt_path = os.path.join(dir_, f\"{base}.txt\")\n",
    "        srt_path = os.path.join(dir_, f\"{base}.srt\")\n",
    "        if os.path.exists(txt_path) and os.path.exists(srt_path):\n",
    "            with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                blocks = f.read().strip().split(\"\\n\\n\")\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                timed_transcript = f.read().strip()\n",
    "            segments = []\n",
    "            for block in blocks:\n",
    "                lines = block.split(\"\\n\")\n",
    "                if len(lines) >= 3:\n",
    "                    times = lines[1].split(\" --> \")\n",
    "                    start = _parse_srt_time(times[0])\n",
    "                    end = _parse_srt_time(times[1])\n",
    "                    text = \" \".join(lines[2:])\n",
    "                    segments.append({\"start\": start, \"end\": end, \"text\": text})\n",
    "            return timed_transcript, segments\n",
    "        self.display_message(\"system\", \"üîç Running Whisper transcription...\")\n",
    "        model = whisper.load_model(\"medium\")\n",
    "        result = model.transcribe(video_path, verbose=True)\n",
    "        segments = []\n",
    "        timed_transcript = \"\"\n",
    "        for seg in result['segments']:\n",
    "            segments.append({\"start\": seg['start'], \"end\": seg['end'], \"text\": seg['text']})\n",
    "            timed_transcript += f\"[{seg['start']:.2f} - {seg['end']:.2f}] {seg['text']}\\n\"\n",
    "        with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, seg in enumerate(segments):\n",
    "                f.write(f\"{i+1}\\n{format_time(seg['start'])} --> {format_time(seg['end'])}\\n{seg['text']}\\n\\n\")\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(timed_transcript)\n",
    "        return timed_transcript, segments\n",
    "\n",
    "    def send_message(self):\n",
    "        user_input = self.user_entry.get()\n",
    "        if user_input.strip().lower() == \"exit\":\n",
    "            self.root.destroy()\n",
    "            return\n",
    "        self.display_message(\"user\", user_input)\n",
    "        self.user_entry.delete(0, tk.END)\n",
    "        self.user_entry.configure(state=\"disabled\")\n",
    "        self.send_btn.configure(state=\"disabled\")\n",
    "\n",
    "        def run_bot():\n",
    "            if user_input.lower().startswith(\"video clipping:\"):\n",
    "                query = user_input[len(\"video clipping:\"):].strip()\n",
    "                response = chain_clipping.invoke({\"query\": query, \"transcript\": self.transcript_segments})\n",
    "                video_clips = []\n",
    "                for line in response.strip().split(\"\\n\"):\n",
    "                    if line.startswith(\"- Range: \"):\n",
    "                        parts = line[len(\"- Range: \"):].split(\" - \")\n",
    "                        try:\n",
    "                            start = float(parts[0])\n",
    "                            end = float(parts[1])\n",
    "                            msg, clip_path = self.clip_video(start, end)\n",
    "                            #self.display_message(\"Vivi\", msg)\n",
    "                            if clip_path:\n",
    "                                video_clips.append(clip_path)\n",
    "                        except:\n",
    "                            continue\n",
    "                if video_clips:\n",
    "                    msg, out = concatenate_videos(video_clips, output_filepath=os.path.join(os.path.dirname(self.video_path), \"final_output.mp4\"))\n",
    "                    self.display_message(\"Vivi\", msg)\n",
    "            else:\n",
    "                response = chain_general.invoke({\n",
    "                    \"context\": self.context,\n",
    "                    \"question\": user_input,\n",
    "                    \"transcript\": self.transcript_segments\n",
    "                })\n",
    "                self.typewriter_effect(\"Vivi\", response)\n",
    "                self.context += f\"\\nUser: {user_input}\\nAI: {response}\\n\"\n",
    "            self.user_entry.configure(state=\"normal\")\n",
    "            self.send_btn.configure(state=\"normal\")\n",
    "            self.user_entry.focus()\n",
    "\n",
    "        threading.Thread(target=run_bot).start()\n",
    "\n",
    "    def browse_video(self):\n",
    "        self.video_path = filedialog.askopenfilename(filetypes=[(\"Video Files\", \"*.mp4 *.mov *.avi\")])\n",
    "        if not self.video_path:\n",
    "            return\n",
    "        self.display_message(\"system\", f\"üìÅ Selected video: {os.path.basename(self.video_path)}\")\n",
    "        def process_video():\n",
    "            self.full_transcript_text, self.transcript_segments = self.transcribe_video(self.video_path)\n",
    "            self.display_message(\"system\", \"‚úÖ Transcription completed!\")\n",
    "            self.progress_var.set(0)\n",
    "        threading.Thread(target=process_video).start()\n",
    "\n",
    "    def play_input_video(self):\n",
    "        if not self.video_path:\n",
    "            self.display_message(\"system\", \"‚ö†Ô∏è No video uploaded yet. Upload a video first.\")\n",
    "            return\n",
    "        if self.audio_process and self.audio_process.poll() is None:\n",
    "            self.audio_process.terminate()\n",
    "        try:\n",
    "            self.audio_process = subprocess.Popen([\"ffplay\", \"-autoexit\", \"-loglevel\", \"quiet\", self.video_path])\n",
    "        except Exception as e:\n",
    "            self.display_message(\"system\", f\"‚ùå Failed to play input video: {e}\")\n",
    "\n",
    "    def play_final_video(self):\n",
    "        final_output = os.path.join(os.path.dirname(self.video_path), \"final_output.mp4\")\n",
    "        if not os.path.exists(final_output):\n",
    "            self.display_message(\"system\", \"‚ö†Ô∏è No final video available. Generate a clipped video first.\")\n",
    "            return\n",
    "        if self.audio_process and self.audio_process.poll() is None:\n",
    "            self.audio_process.terminate()\n",
    "        try:\n",
    "            self.audio_process = subprocess.Popen([\"ffplay\", \"-autoexit\", \"-loglevel\", \"quiet\", final_output])\n",
    "        except Exception as e:\n",
    "            self.display_message(\"system\", f\"‚ùå Failed to play final video: {e}\")\n",
    "\n",
    "    def run(self):\n",
    "        self.root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = ViviChatbot()\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191882b6-6647-4e60-941d-6dbb12d40aec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Video_AIbot",
   "language": "python",
   "name": "video_aibot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
